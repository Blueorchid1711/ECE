{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK4ia4Iwwbtd4e13ACMUrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blueorchid1711/ECE/blob/main/Covid_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAUnDaCxhnpR",
        "outputId": "b54f7862-8bfc-46ba-c29b-800b5221c054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/pranavraikokte/covid19-image-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 158M/158M [00:06<00:00, 24.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/pranavraikokte/covid19-image-dataset/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pranavraikokte/covid19-image-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73bee05b"
      },
      "source": [
        "# Task\n",
        "Build a CNN model with TensorFlow for a hackathon using the dataset located at \"kaggle://competitions/dogs-vs-cats/data\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5517fe6a"
      },
      "source": [
        "## Load and preprocess the data\n",
        "\n",
        "### Subtask:\n",
        "Load the image data downloaded from Kaggle and prepare it for training the CNN model. This includes tasks like splitting the data into training and validation sets, resizing images, and applying data augmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6137da8"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries and define constants for image loading and preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d95ea26"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = '/root/.cache/kagglehub/datasets/pranavraikokte/covid19-image-dataset/versions/2/Covid19-dataset/train'\n",
        "test_dir = '/root/.cache/kagglehub/datasets/pranavraikokte/covid19-image-dataset/versions/2/Covid19-dataset/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3ca0f9"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the training and validation datasets from the specified directories using `image_dataset_from_directory` and apply a validation split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39b1a19b",
        "outputId": "3e06abdc-6bc2-41a8-967b-3bbdd1ddedde"
      },
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 251 files belonging to 3 classes.\n",
            "Using 201 files for training.\n",
            "Found 251 files belonging to 3 classes.\n",
            "Using 50 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68e13967"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply data augmentation and rescaling to the training dataset, and only rescaling to the validation dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b4e6e74",
        "outputId": "f7615fdf-6271-4da0-cc0f-6e1f4fadccf3"
      },
      "source": [
        "data_augmentation = tf.keras.Sequential(\n",
        "  [\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(IMG_HEIGHT,\n",
        "                                  IMG_WIDTH,\n",
        "                                  3)),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "  ]\n",
        ")\n",
        "\n",
        "rescale = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "train_ds = train_ds.map(lambda x, y: (rescale(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (rescale(x), y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53ea00dc"
      },
      "source": [
        "**Reasoning**:\n",
        "Configure the datasets for performance using `cache()` and `prefetch()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBYsgtLliP6B"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fe9c015"
      },
      "source": [
        "## Build the cnn model\n",
        "\n",
        "### Subtask:\n",
        "Define the architecture of the Convolutional Neural Network using TensorFlow. This involves adding layers such as convolutional layers, pooling layers, and dense layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ebf8f6"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary layers from tensorflow.keras.layers and define the CNN model architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "414cd554",
        "outputId": "138d9571-bfbe-4edf-e10a-85b18af50bba"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling\n",
        "\n",
        "num_classes = len(train_ds.class_names)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'_PrefetchDataset' object has no attribute 'class_names'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-156119923.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRescaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model = tf.keras.Sequential([\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_PrefetchDataset' object has no attribute 'class_names'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0960c486"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `_PrefetchDataset` object does not have a `class_names` attribute. The `class_names` attribute is available on the dataset object before applying `cache()` and `prefetch()`. I will get the class names from the original `train_ds` before applying `cache()` and `prefetch()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMpsPw4miXrZ",
        "outputId": "bed642bb-4e0a-4776-8991-c1cde327fa86"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling\n",
        "\n",
        "# Get class names before applying cache() and prefetch()\n",
        "original_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "num_classes = len(original_train_ds.class_names)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 251 files belonging to 3 classes.\n",
            "Using 201 files for training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ca7c248"
      },
      "source": [
        "## Compile the model\n",
        "\n",
        "### Subtask:\n",
        "Configure the model for training by specifying the optimizer, loss function, and metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6c0f53b"
      },
      "source": [
        "**Reasoning**:\n",
        "Compile the defined CNN model with the specified optimizer, loss function, and metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96808dc2"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4b9d0d"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the CNN model using the prepared training data and validate its performance on the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30b17e16"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the compiled model using the training and validation datasets for a specified number of epochs and store the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8607a6fd",
        "outputId": "422777cf-2ed7-4042-acae-a775f4377b0d"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.4567 - loss: 1.0968 - val_accuracy: 0.4800 - val_loss: 1.0601\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 405ms/step - accuracy: 0.4459 - loss: 1.0757 - val_accuracy: 0.4800 - val_loss: 1.0591\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 414ms/step - accuracy: 0.4459 - loss: 1.0700 - val_accuracy: 0.4800 - val_loss: 1.0498\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 520ms/step - accuracy: 0.4459 - loss: 1.0580 - val_accuracy: 0.4800 - val_loss: 1.0279\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 445ms/step - accuracy: 0.4459 - loss: 1.0244 - val_accuracy: 0.6600 - val_loss: 0.9695\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 396ms/step - accuracy: 0.5613 - loss: 0.9324 - val_accuracy: 0.8000 - val_loss: 0.8357\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 393ms/step - accuracy: 0.7297 - loss: 0.7527 - val_accuracy: 0.7800 - val_loss: 0.7012\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 435ms/step - accuracy: 0.7781 - loss: 0.5573 - val_accuracy: 0.8200 - val_loss: 0.6230\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 501ms/step - accuracy: 0.8287 - loss: 0.4651 - val_accuracy: 0.8600 - val_loss: 0.5442\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 396ms/step - accuracy: 0.8841 - loss: 0.3718 - val_accuracy: 0.8800 - val_loss: 0.5695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f13787a"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Assess the performance of the trained model using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f88511"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the evaluate method of the trained model with the validation dataset to calculate the loss and accuracy, then print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36843773",
        "outputId": "32dbef18-e41b-4a7f-d449-882950ec7506"
      },
      "source": [
        "loss, accuracy = model.evaluate(val_ds)\n",
        "print(f\"Validation Loss: {loss}\")\n",
        "print(f\"Validation Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.8888 - loss: 0.5447\n",
            "Validation Loss: 0.5694524645805359\n",
            "Validation Accuracy: 0.8799999952316284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b621b9a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The CNN model achieved a validation accuracy of approximately 88% and a validation loss of approximately 0.57 after training for 10 epochs.\n",
        "*   The training and validation loss generally decreased over the epochs, while training and validation accuracy generally increased.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Consider training the model for more epochs or implementing early stopping to potentially improve performance and prevent overfitting.\n",
        "*   Explore more advanced CNN architectures or transfer learning techniques to further enhance the model's accuracy.\n"
      ]
    }
  ]
}